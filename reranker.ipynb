{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00fa7e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed:\n",
    "# !pip install fastembed numpy tqdm\n",
    "\n",
    "import json, pathlib\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fastembed import SparseTextEmbedding  # MiniCOIL lives here\n",
    "import torch\n",
    "from sentence_transformers import CrossEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab47619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your subset or full prepared set\n",
    "DATA_DIR = \"prepared/msmarco-dev-subset-1000-plus-50000-neg\"   # or \"prepared/msmarco-dev\"\n",
    "\n",
    "# Path to the previously saved dense run (top-10 per query)\n",
    "RUNS_DIR = \"runs\"\n",
    "IN_JSON  = pathlib.Path(RUNS_DIR) / f\"{pathlib.Path(DATA_DIR).name}_dense_top10.json\"  # set this to your saved file\n",
    "# If you only have TSV instead, set IN_TSV and leave IN_JSON=None\n",
    "IN_TSV   = None  # e.g., pathlib.Path(RUNS_DIR) / f\"{pathlib.Path(DATA_DIR).name}_dense_top10.tsv\"\n",
    "\n",
    "TOPK = 10\n",
    "OUT_JSON = pathlib.Path(RUNS_DIR) / f\"{pathlib.Path(DATA_DIR).name}_dense_top{TOPK}_rerank_lateint.json\"\n",
    "OUT_TSV  = pathlib.Path(RUNS_DIR) / f\"{pathlib.Path(DATA_DIR).name}_dense_top{TOPK}_rerank_lateint.tsv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d847cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries.tsv\n",
    "queries: Dict[str, str] = {}\n",
    "with open(pathlib.Path(DATA_DIR) / \"queries.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        qid, qtext = line.rstrip(\"\\n\").split(\"\\t\", 1)\n",
    "        queries[qid] = qtext\n",
    "\n",
    "# qrels.tsv (optional for eval later)\n",
    "qrels: Dict[str, Dict[str,int]] = {}\n",
    "with open(pathlib.Path(DATA_DIR) / \"qrels.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        qid, _, docid, rel = line.strip().split(\"\\t\")\n",
    "        qrels.setdefault(qid, {})[docid] = int(rel)\n",
    "\n",
    "# corpus.jsonl (for text lookup)\n",
    "corpus: Dict[str, Dict[str,str]] = {}\n",
    "with open(pathlib.Path(DATA_DIR) / \"corpus.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec = json.loads(line)\n",
    "        corpus[rec[\"_id\"]] = {\"title\": rec.get(\"title\",\"\"), \"text\": rec.get(\"text\",\"\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0077cc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_run_json(path: pathlib.Path) -> Dict[str, List[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_run_tsv(path: pathlib.Path) -> Dict[str, List[str]]:\n",
    "    res: Dict[str, List[str]] = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) < 2: \n",
    "                continue\n",
    "            qid, docid = parts[0], parts[1]\n",
    "            res.setdefault(qid, []).append(docid)\n",
    "    return res\n",
    "\n",
    "if IN_JSON and pathlib.Path(IN_JSON).exists():\n",
    "    base_run = load_run_json(IN_JSON)\n",
    "elif IN_TSV and pathlib.Path(IN_TSV).exists():\n",
    "    base_run = load_run_tsv(IN_TSV)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No input run found. Set IN_JSON or IN_TSV correctly.\")\n",
    "\n",
    "# Keep only queries we have text for (and clamp to TOPK if needed)\n",
    "for qid, docs in list(base_run.items()):\n",
    "    base_run[qid] = docs[:TOPK]\n",
    "len(base_run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c257ed74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ec9f097d8d457d96a4412d28317fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23aa7fea91b24612990d942c4b870eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0084e5e1064986924724370120ea29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da3298ee7214595b6c568bc1d419ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d0744c1bc54c06a48679aacba243f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stopwords.txt:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e38f06cc5a940368cd7610dbd946902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sarab\\AppData\\Local\\Temp\\fastembed_cache\\models--Qdrant--minicoil-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c086baeb6d44d3b16b39e83e3a13de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "minicoil.triplet.model.vocab: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df45fdd81e14e38a2f2bb9b317bf69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "minicoil.triplet.model.npy:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2698392f33a444f914270884a758a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/130M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize MiniCOIL\n",
    "minicoil = SparseTextEmbedding(model_name=\"Qdrant/minicoil-v1\")  # downloads from HF\n",
    "\n",
    "def sparse_dot(q_idx: np.ndarray, q_val: np.ndarray, d_idx: np.ndarray, d_val: np.ndarray) -> float:\n",
    "    \"\"\"Efficient sparse dot: intersect by walking two sorted index arrays.\"\"\"\n",
    "    i = j = 0\n",
    "    score = 0.0\n",
    "    # (FastEmbed returns indices sorted; if not, np.argsort first)\n",
    "    while i < len(q_idx) and j < len(d_idx):\n",
    "        if q_idx[i] == d_idx[j]:\n",
    "            score += float(q_val[i]) * float(d_val[j])\n",
    "            i += 1; j += 1\n",
    "        elif q_idx[i] < d_idx[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return score\n",
    "\n",
    "def score_minicoil(query_text: str, doc_texts: List[str]) -> np.ndarray:\n",
    "    # Embed query (one vector)\n",
    "    q_emb = list(minicoil.query_embed(query_text))[0]   # SparseEmbedding(values, indices)\n",
    "    q_idx = q_emb.indices\n",
    "    q_val = q_emb.values\n",
    "\n",
    "    # Embed documents (iterator over SparseEmbedding)\n",
    "    d_embs = list(minicoil.embed(doc_texts))\n",
    "\n",
    "    scores = np.zeros(len(doc_texts), dtype=np.float32)\n",
    "    for k, de in enumerate(d_embs):\n",
    "        scores[k] = sparse_dot(q_idx, q_val, de.indices, de.values)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ceaf480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking with MiniCOIL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [03:31<00:00,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: runs\\msmarco-dev-subset-1000-plus-50000-neg_dense_top10_rerank_lateint.json\n",
      "Saved: runs\\msmarco-dev-subset-1000-plus-50000-neg_dense_top10_rerank_lateint.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reranked: Dict[str, List[str]] = {}\n",
    "qids_eval = [qid for qid in base_run if qid in queries]\n",
    "\n",
    "for qid in tqdm(qids_eval, desc=\"Reranking with MiniCOIL\"):\n",
    "    cand_ids = base_run[qid]\n",
    "    cand_texts = [ (corpus[d][\"title\"] + \" \" + corpus[d][\"text\"]).strip() for d in cand_ids ]\n",
    "    # optional hard cap on very long passages (speed); MiniCOIL is robust\n",
    "    cand_texts = [t[:4096] for t in cand_texts]\n",
    "    s = score_minicoil(queries[qid], cand_texts)\n",
    "    order = np.argsort(-s)\n",
    "    reranked[qid] = [cand_ids[i] for i in order]\n",
    "\n",
    "# Save\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(reranked, f, indent=2)\n",
    "\n",
    "with open(OUT_TSV, \"w\", encoding=\"utf-8\") as f:\n",
    "    for qid, docs in reranked.items():\n",
    "        for rank, docid in enumerate(docs, start=1):\n",
    "            f.write(f\"{qid}\\t{docid}\\t{rank}\\n\")\n",
    "\n",
    "print(\"Saved:\", OUT_JSON)\n",
    "print(\"Saved:\", OUT_TSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f6973da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: {'queries': 1000, 'nDCG@10': np.float64(0.921747962526825), 'MRR@10': np.float64(0.9036996031746032), 'Recall@10': np.float64(0.9828333333333332), 'Precision@10': np.float64(0.10340000000000002), 'MAP@10': np.float64(0.9004077380952381)}\n",
      "After : {'queries': 1000, 'nDCG@10': np.float64(0.8628394472241399), 'MRR@10': np.float64(0.8268853174603176), 'Recall@10': np.float64(0.9828333333333332), 'Precision@10': np.float64(0.10340000000000002), 'MAP@10': np.float64(0.8217142857142857)}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def ndcg_at_k(ranked, truth, k=10):\n",
    "    dcg = 0.0\n",
    "    for i,d in enumerate(ranked[:k], start=1):\n",
    "        rel = truth.get(d,0)\n",
    "        if rel>0: dcg += rel / math.log2(i+1)\n",
    "    idcg = sum(rel/math.log2(i+1) for i,rel in enumerate(sorted(truth.values(), reverse=True)[:k], start=1))\n",
    "    return dcg/idcg if idcg>0 else 0.0\n",
    "\n",
    "def mrr_at_k(ranked, truth, k=10):\n",
    "    for i,d in enumerate(ranked[:k], start=1):\n",
    "        if truth.get(d,0)>0: return 1.0/i\n",
    "    return 0.0\n",
    "\n",
    "def recall_at_k(ranked, truth, k=10):\n",
    "    rels = [d for d,r in truth.items() if r>0]\n",
    "    if not rels: return 0.0\n",
    "    return sum(1 for d in ranked[:k] if truth.get(d,0)>0)/len(rels)\n",
    "\n",
    "def precision_at_k(ranked, truth, k=10):\n",
    "    return sum(1 for d in ranked[:k] if truth.get(d,0)>0)/k\n",
    "\n",
    "def ap_at_k(ranked, truth, k=10):\n",
    "    rels = [d for d,r in truth.items() if r>0]\n",
    "    if not rels: return 0.0\n",
    "    ap,h = 0.0,0\n",
    "    for i,d in enumerate(ranked[:k], start=1):\n",
    "        if truth.get(d,0)>0:\n",
    "            h += 1; ap += h/i\n",
    "    return ap/len(rels)\n",
    "def eval_run(run):\n",
    "    qids = [qid for qid in run if qid in qrels]\n",
    "    ndcg = np.mean([ndcg_at_k(run[qid], qrels[qid], 10) for qid in qids])\n",
    "    mrr  = np.mean([mrr_at_k(run[qid],  qrels[qid], 10) for qid in qids])\n",
    "    rec  = np.mean([recall_at_k(run[qid],qrels[qid],10) for qid in qids])\n",
    "    prec = np.mean([precision_at_k(run[qid],qrels[qid],10) for qid in qids])\n",
    "    mapk = np.mean([ap_at_k(run[qid],   qrels[qid],10) for qid in qids])\n",
    "    return {\"queries\": len(qids), \"nDCG@10\":ndcg, \"MRR@10\":mrr, \"Recall@10\":rec, \"Precision@10\":prec, \"MAP@10\":mapk}\n",
    "\n",
    "print(\"Before:\", eval_run(base_run))\n",
    "print(\"After :\", eval_run(reranked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54c67cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Previously saved dense retrieval results (top-k per query)\n",
    "RUNS_DIR = \"runs\"\n",
    "IN_JSON  = pathlib.Path(RUNS_DIR) / f\"{pathlib.Path(DATA_DIR).name}_dense_top10.json\"  # set to your file\n",
    "IN_TSV   = None  # e.g., pathlib.Path(RUNS_DIR) / f\"{pathlib.Path(DATA_DIR).name}_dense_top10.tsv\"\n",
    "\n",
    "TOPK = 10\n",
    "\n",
    "# Output\n",
    "OUT_JSON = pathlib.Path(RUNS_DIR) / f\"{pathlib.Path(DATA_DIR).name}_dense_top{TOPK}_rerank_crossenc.json\"\n",
    "OUT_TSV  = pathlib.Path(RUNS_DIR) / f\"{pathlib.Path(DATA_DIR).name}_dense_top{TOPK}_rerank_crossenc.tsv\"\n",
    "\n",
    "# Cross-Encoder model (fast, MS MARCO-tuned)\n",
    "CE_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "MAX_SEQ_LEN = 256   # tradeoff: 256~384 usually good for passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0aa0c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bdb3546e3d4062b932870e47298486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sarab\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L-6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fd58a99cf245adbe6c64ec10e810d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f243de4023e483fbde646dddece4f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410b45c7d2494a22bab794705c409f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3350d2d01f54018ab49638de3cf6deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36450e282bc4c56bd914488478d0cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf1abd06158471ca1130e7c48ece7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking (Cross-Encoder):   0%|          | 0/1000 [00:00<?, ?it/s]c:\\Users\\sarab\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  try:\n",
      "Reranking (Cross-Encoder): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [02:43<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: runs\\msmarco-dev-subset-1000-plus-50000-neg_dense_top10_rerank_crossenc.json\n",
      "Saved: runs\\msmarco-dev-subset-1000-plus-50000-neg_dense_top10_rerank_crossenc.tsv\n"
     ]
    }
   ],
   "source": [
    "# Initialize Cross-Encoder\n",
    "ce = CrossEncoder(CE_MODEL, device=DEVICE, max_length=MAX_SEQ_LEN)\n",
    "\n",
    "reranked_ce: Dict[str, List[str]] = {}\n",
    "qids_eval = [qid for qid in base_run if qid in queries]\n",
    "\n",
    "for qid in tqdm(qids_eval, desc=\"Reranking (Cross-Encoder)\"):\n",
    "    cand_ids = base_run[qid]\n",
    "    # Build (query, passage) pairs\n",
    "    cand_texts = [ (corpus[d][\"title\"] + \" \" + corpus[d][\"text\"]).strip() for d in cand_ids ]\n",
    "    # Truncate passages hard to keep within MAX_SEQ_LEN after tokenization (cheap guard)\n",
    "    cand_texts = [t[:4096] for t in cand_texts]\n",
    "\n",
    "    pairs = [(queries[qid], t) for t in cand_texts]\n",
    "    scores = ce.predict(pairs, batch_size=BATCH_SIZE)  # higher = more relevant\n",
    "    order = np.argsort(-scores)\n",
    "    reranked_ce[qid] = [cand_ids[i] for i in order]\n",
    "\n",
    "# Save runs\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(reranked_ce, f, indent=2)\n",
    "\n",
    "with open(OUT_TSV, \"w\", encoding=\"utf-8\") as f:\n",
    "    for qid, docs in reranked_ce.items():\n",
    "        for rank, docid in enumerate(docs, start=1):\n",
    "            f.write(f\"{qid}\\t{docid}\\t{rank}\\n\")\n",
    "\n",
    "print(\"Saved:\", OUT_JSON)\n",
    "print(\"Saved:\", OUT_TSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eaaf9bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: {'queries': 1000, 'nDCG@10': np.float64(0.921747962526825), 'MRR@10': np.float64(0.9036996031746032), 'Recall@10': np.float64(0.9828333333333332), 'Precision@10': np.float64(0.10340000000000002), 'MAP@10': np.float64(0.9004077380952381)}\n",
      "After : {'queries': 1000, 'nDCG@10': np.float64(0.9537821760077454), 'MRR@10': np.float64(0.9455095238095239), 'Recall@10': np.float64(0.9828333333333332), 'Precision@10': np.float64(0.10340000000000002), 'MAP@10': np.float64(0.9427373015873015)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", eval_run(base_run))\n",
    "print(\"After :\", eval_run(reranked_ce))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f251c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QID: 1090086\n",
      "Query: technology working group definition\n",
      "============================================================================================================================================\n",
      "Dense (before)                                | MiniCOIL rerank                               | Cross-Encoder rerank                         \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      " 1. ‚úó 598183 Current Practice and Perceptions of Grou... |  1. ‚úó 6756074 Applied information technology (AIT) is ... |  1. ‚úó 8090876 What is a person who studies technology ...\n",
      " 2. ‚úó 8090876 What is a person who studies technology ... |  2. ‚úó 8090876 What is a person who studies technology ... |  2. ‚úó 7784310 Collaborative software is a broad concep...\n",
      " 3. ‚úó 371375 , predicts that within the next ten year... |  3. ‚úó 598183 Current Practice and Perceptions of Grou... |  3. ‚úó 6756074 Applied information technology (AIT) is ...\n",
      " 4. ‚úó 6756074 Applied information technology (AIT) is ... |  4. ‚úó 371375 , predicts that within the next ten year... |  4. ‚úó 2569520 Technology is often a consequence of sci...\n",
      " 5. ‚úó 2569520 Technology is often a consequence of sci... |  5. ‚úó 557802 Description. Group Dynamics: Theory, Res... |  5. ‚úó 557802 Description. Group Dynamics: Theory, Res...\n",
      "\n",
      "QID: 1083721\n",
      "Query: what does hair tint do\n",
      "============================================================================================================================================\n",
      "Dense (before)                                | MiniCOIL rerank                               | Cross-Encoder rerank                         \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      " 1. ‚úì 7134814 Hair tint is a type of hair dye that cha... |  1. ‚úì 7134814 Hair tint is a type of hair dye that cha... |  1. ‚úì 7134814 Hair tint is a type of hair dye that cha...\n",
      " 2. ‚úó 6742020 In more recent years, technology has bee... |  2. ‚úó 5842492 You are more prone to compromise on your... |  2. ‚úó 3764660 Benefit aestheticians can give you a fla...\n",
      " 3. ‚úó 5136135 Visibility tint lenses have a tiny bit o... |  3. ‚úó 5136135 Visibility tint lenses have a tiny bit o... |  3. ‚úó 5136135 Visibility tint lenses have a tiny bit o...\n",
      " 4. ‚úó 3764660 Benefit aestheticians can give you a fla... |  4. ‚úó 137644 Excessive or unwanted hair that grows on... |  4. ‚úó 3267392 It's the fact that with hair across the ...\n",
      " 5. ‚úó 137644 Excessive or unwanted hair that grows on... |  5. ‚úó 6289336 In addition to heredity, poor nutrition,... |  5. ‚úó 137644 Excessive or unwanted hair that grows on...\n",
      "\n",
      "QID: 1086354\n",
      "Query: what can help dogs sleep\n",
      "============================================================================================================================================\n",
      "Dense (before)                                | MiniCOIL rerank                               | Cross-Encoder rerank                         \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      " 1. ‚úó 7110953 Can I Give My Dog Sleeping Pills? Answer... |  1. ‚úì 7110954 For senior dogs with aches and pains, a ... |  1. ‚úó 7110953 Can I Give My Dog Sleeping Pills? Answer...\n",
      " 2. ‚úó 3732094 Before you introduce the new dog bed to ... |  2. ‚úó 7110953 Can I Give My Dog Sleeping Pills? Answer... |  2. ‚úì 7110954 For senior dogs with aches and pains, a ...\n",
      " 3. ‚úì 7110954 For senior dogs with aches and pains, a ... |  3. ‚úó 3732094 Before you introduce the new dog bed to ... |  3. ‚úó 3732094 Before you introduce the new dog bed to ...\n",
      " 4. ‚úó 599709 The roots for the phrase 'let sleeping d... |  4. ‚úó 2061860 How to help your school-aged child sleep... |  4. ‚úó 1904097 There are foods that can be comforting, ...\n",
      " 5. ‚úó 5767357 Furthermore, it is an excellent remedy f... |  5. ‚úó 599709 The roots for the phrase 'let sleeping d... |  5. ‚úó 7088908 A: A dog with diabetes can eat a variety...\n"
     ]
    }
   ],
   "source": [
    "def compare_three(qid, run_dense, run_minicoil, run_ce, k=5):\n",
    "    print(f\"\\nQID: {qid}\")\n",
    "    print(\"Query:\", queries[qid])\n",
    "    print(\"=\"*140)\n",
    "    print(f\"{'Dense (before)':45} | {'MiniCOIL rerank':45} | {'Cross-Encoder rerank':45}\")\n",
    "    print(\"-\"*140)\n",
    "    for i in range(k):\n",
    "        # dense\n",
    "        docid_b = run_dense[qid][i]\n",
    "        rel_b = qrels.get(qid, {}).get(docid_b, 0)\n",
    "        mark_b = \"‚úì\" if rel_b>0 else \"‚úó\"\n",
    "        text_b = (corpus[docid_b][\"text\"][:40]).replace(\"\\n\",\" \")\n",
    "        left = f\"{i+1:2d}. {mark_b} {docid_b} {text_b}...\"\n",
    "\n",
    "        # minicoil\n",
    "        docid_m = run_minicoil[qid][i]\n",
    "        rel_m = qrels.get(qid, {}).get(docid_m, 0)\n",
    "        mark_m = \"‚úì\" if rel_m>0 else \"‚úó\"\n",
    "        text_m = (corpus[docid_m][\"text\"][:40]).replace(\"\\n\",\" \")\n",
    "        mid = f\"{i+1:2d}. {mark_m} {docid_m} {text_m}...\"\n",
    "\n",
    "        # cross-encoder\n",
    "        docid_c = run_ce[qid][i]\n",
    "        rel_c = qrels.get(qid, {}).get(docid_c, 0)\n",
    "        mark_c = \"‚úì\" if rel_c>0 else \"‚úó\"\n",
    "        text_c = (corpus[docid_c][\"text\"][:40]).replace(\"\\n\",\" \")\n",
    "        right = f\"{i+1:2d}. {mark_c} {docid_c} {text_c}...\"\n",
    "\n",
    "        print(f\"{left:45} | {mid:45} | {right:45}\")\n",
    "\n",
    "import random\n",
    "\n",
    "def sample_three(run_dense, run_minicoil, run_ce, n=3):\n",
    "    qids = list(run_dense.keys())\n",
    "    random.shuffle(qids)\n",
    "    shown = 0\n",
    "    for qid in qids:\n",
    "        if qid not in run_minicoil or qid not in run_ce:\n",
    "            continue\n",
    "        # skip queries with no relevant docs\n",
    "        if not qrels.get(qid, {}):\n",
    "            continue\n",
    "        compare_three(qid, run_dense, run_minicoil, run_ce, k=5)\n",
    "        shown += 1\n",
    "        if shown >= n:\n",
    "            break\n",
    "\n",
    "# Example: show 3 queries side-by-side\n",
    "sample_three(base_run, reranked, reranked_ce, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370c807",
   "metadata": {},
   "source": [
    "üîπ QID: 1090086\n",
    "\n",
    "Query: technology working group definition\n",
    "\n",
    "Dense (before): Brings in documents broadly about technology or groups, but nothing that clearly defines ‚Äútechnology working group.‚Äù\n",
    "\n",
    "MiniCOIL rerank: Tries to reorder based on token-level matches like technology and group. It still doesn‚Äôt capture the intended ‚Äúdefinition,‚Äù so results shuffle around but remain off-target.\n",
    "\n",
    "Cross-Encoder rerank: Uses full query‚Äìdocument interactions. The top doc is still not great (mentions ‚Äúperson who studies technology‚Äù), but notice by rank 2 it surfaces ‚ÄúCollaborative software‚Ä¶‚Äù which is closer to a working group sense.\n",
    "üëâ Neither stage fully nails the definition, but cross-encoder is nudging results slightly closer to intent.\n",
    "\n",
    "üîπ QID: 1083721\n",
    "\n",
    "Query: what does hair tint do\n",
    "\n",
    "Dense (before): Immediately retrieves the relevant passage (‚úì: Hair tint is a type of hair dye‚Ä¶). Dense vectors are already strong for this easy factual query.\n",
    "\n",
    "MiniCOIL rerank: Keeps the same relevant doc at rank 1. Lower ranks shuffle (sometimes in irrelevant beauty/vision tint directions), because token-level overlap can be misleading (tint also appears in lens tint).\n",
    "\n",
    "Cross-Encoder rerank: Keeps the correct answer firmly at rank 1, and also promotes other cosmetically relevant docs higher than e.g. vision tint.\n",
    "üëâ For simple factual questions with a clear keyword, all methods succeed, but cross-encoder preserves and stabilizes relevance.\n",
    "\n",
    "üîπ QID: 1086354\n",
    "\n",
    "Query: what can help dogs sleep\n",
    "\n",
    "Dense (before): Top doc is a Q&A about sleeping pills for dogs (not what the query asked), while the actually relevant doc (For senior dogs with aches and pains‚Ä¶ can help dogs sleep better) is at rank 3.\n",
    "\n",
    "MiniCOIL rerank: Boosts that relevant doc (senior dogs with aches and pains‚Ä¶) to rank 1 ‚Äî nice example where sparse token-level matching helps (help dogs sleep appears literally).\n",
    "\n",
    "Cross-Encoder rerank: Drops the relevant doc back to rank 2, because it weights semantic context: the ‚Äúsleeping pills‚Äù doc (rank 1) seems closer to a ‚Äúdirect answer.‚Äù\n",
    "üëâ This shows the trade-off: MiniCOIL nailed literal matching and surfaced the correct doc, while cross-encoder misjudged slightly (preferring the pills doc).\n",
    "\n",
    "üß© Takeaways\n",
    "\n",
    "Dense retrieval is strong at topical similarity but not exact intent.\n",
    "\n",
    "MiniCOIL reranker helps when queries are literal and the words match (like help dogs sleep), but can be noisy when terms are polysemous (tint).\n",
    "\n",
    "Cross-Encoder reranker usually wins on average because it models full sentence interactions, but it can over-prioritize superficially ‚Äúanswer-like‚Äù docs (e.g., pills vs natural remedies).\n",
    "\n",
    "That‚Äôs why in practice, many systems use:\n",
    "\n",
    "Dense retriever (broad coverage)\n",
    "\n",
    "Sparse reranker (MiniCOIL/ColBERT) for token-level matches\n",
    "\n",
    "Cross-Encoder reranker for final precision\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
